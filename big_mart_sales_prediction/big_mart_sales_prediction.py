# -*- coding: utf-8 -*-
"""Big Mart Sales Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gGXS2dPjXY1kNr40jyXNYePbI0vBodpH
"""

import numpy as np
print(np.__version__)

"""# Pre-processing Steps

<ol>1. Filling the missing values</ol>
<ol>2. Converting categories to numbers</ol>
<ol>3. Bring all the variables in range 0 to 1</ol>
"""

# importing required libraries
import pandas as pd
# check version on pandas
print('Version of pandas:', pd.__version__)

# reading the loan prediction data
train = pd.read_csv('train_XnW6LSF.csv')
test = pd.read_csv('test_FewQE9B.csv')
sample_sub = pd.read_csv('sample_submission_hP4II7x.csv')

data = train.copy()
print("Shape", data.shape)
# checking missing values in the data
print("Nulls",round((data.isnull().sum()/len(data))*100,2) )
print("DataTypes")
# data types of the variables
print(data.dtypes)
data.head()

#Multiple records of loanids?
data.groupby(['Item_Identifier','Outlet_Identifier']).size().sort_values(ascending=False).head() #No

"""## 1. Filling the missing values

### Categorical Data: Mode
"""

data['Outlet_Size'].fillna(data['Outlet_Size'].mode()[0], inplace=True)

# data.isnull().sum()

"""### Continuous Data: Mean"""

# filling missing values of continuous variables with mean
data['Item_Weight'].fillna(data['Item_Weight'].mean(), inplace=True)

# checking missing values after imputation
print("Nulls",round((data.isnull().sum()/len(data))*100,2) )

"""## 2. Converting categories to numbers"""

data.Item_Fat_Content.unique()

cat_cols = data.select_dtypes(include=['object']).columns.tolist()
print(cat_cols)
cat_cols.remove('Item_Identifier')
cat_cols.remove('Outlet_Identifier')
cat_cols.remove('Item_Fat_Content')
cat_cols

# converting the categories into numbers using map function
dict1 = {'Low Fat': 0, 'low fat': 0, 'LF':0, 'Regular':1, 'reg': 1}
data['Item_Fat_Content'] = data['Item_Fat_Content'].map(dict1)
data.Item_Fat_Content.unique()

data_item_id = data["Item_Identifier"].copy()
data_outlet_id = data["Outlet_Identifier"].copy()
data_target = data['Item_Outlet_Sales'].copy()
model_df = data.drop(["Item_Identifier","Outlet_Identifier","Item_Outlet_Sales"], axis = 1)
num_cols = model_df.select_dtypes(include=['float','int']).columns.tolist()
num_cols
# Convert categorical variables to numeric - one hot encoder

from sklearn.preprocessing import OneHotEncoder


cat_encoder = OneHotEncoder(sparse=False)
model_cat = model_df[cat_cols]
model_cat_1hot = cat_encoder.fit_transform(model_cat)
# model_cat_1hot.toarray()

#     print(cat_encoder.categories_)


# Scale numeric
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
    ('std_scaler', StandardScaler())
])




model_num = model_df.drop(cat_cols, axis = 1)
model_num_tr = num_pipeline.fit_transform(model_num)


from sklearn.compose import ColumnTransformer

num_attribs = list(model_num)
cat_attribs = cat_cols

full_pipeline = ColumnTransformer([
    ('num', num_pipeline, num_attribs),
    ('cat', OneHotEncoder(sparse=False), cat_attribs)
])


model_prepared = full_pipeline.fit_transform(model_df)

trans_df = pd.DataFrame(
    model_prepared,
    columns=num_cols + cat_encoder.get_feature_names().tolist())

trans_df["Item_Identifier"] = data_item_id
trans_df["Outlet_Identifier"] = data_outlet_id
trans_df["Item_Outlet_Sales"] = data_target
cols = list(trans_df)
cols.insert(0, cols.pop(cols.index('Outlet_Identifier')))
cols.insert(0, cols.pop(cols.index('Item_Identifier')))
trans_df = trans_df.loc[:, cols]
trans_df.head()

# saving the pre-processed data
trans_df.to_csv('bigmart_processed_data.csv', index=False)

"""#Solving_Loan_Prediction_problem_using_Neural_Network"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

"""# Steps to build a Neural Network using Keras

<ol>1. Loading the dataset</ol>
<ol>2. Creating training and validation set</ol>
<ol>3. Defining the architecture of the model</ol>
<ol>4. Compiling the model (defining loss function, optimizer)</ol>
<ol>5. Training the model</ol>
<ol>6. Evaluating model performance on training and validation set</ol>

## 1. Loading the dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# importing the required libraries
import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
# %matplotlib inline

# check version on sklearn
print('Version of sklearn:', sklearn.__version__)

# loading the pre-processed dataset
data = pd.read_csv('bigmart_processed_data.csv')

# looking at the first five rows of the dataset
data.head()

# checking missing values
data.isnull().sum()

# checking the data type
data.dtypes

# removing the loan_ID since these are just the unique values
data_item_id = data["Item_Identifier"].copy()
data_outlet_id = data["Outlet_Identifier"].copy()
data = data.drop(["Item_Identifier","Outlet_Identifier"], axis = 1)

# looking at the shape of the data
data.shape

# separating the independent and dependent variables

# storing all the independent variables as X
X = data.drop('Item_Outlet_Sales', axis=1)

# storing the dependent variable as y
y = data['Item_Outlet_Sales']

# shape of independent and dependent variables
X.shape, y.shape

"""## 2. Creating training and validation set"""

# Creating training and validation set

# random state to regenerate the same train and validation set
# test size 0.2 will keep 20% data in validation and remaining 80% in train set

X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=10,test_size=0.2)

# shape of training and validation set
(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)

"""## 3. Defining the architecture of the model"""

# checking the version of keras
import keras
print(keras.__version__)

# checking the version of tensorflow
import tensorflow as tf
print(tf.__version__)

"""### a. Create a model

<img src='https://drive.google.com/uc?id=1iZNZ3kwSHRNf-Irn3DZmMuBb6K-Lro7w'>

### b. Defining different layers

<img src='https://drive.google.com/uc?id=16X6De2hua1XJBe3dfmUUeGTgP6PbXEpc'>

<img src='https://drive.google.com/uc?id=1tsy4B6G0UN4-J4L4roOdoWQiZMUdgw2a'>
"""

# number of input neurons
X_train.shape

# number of features in the data
X_train.shape[1]

# defining input neurons
input_neurons = X_train.shape[1]

"""<img src='https://drive.google.com/uc?id=1xL_hM9rGItZjsZ8Lofwzw_9fZUi4bgJo'>"""

# number of output neurons

# since loan prediction is a binary classification problem, we will have single neuron in the output layer

# define number of output neurons
output_neurons = 1



# number of hidden layers and hidden neurons

# It is a hyperparameter and we can pick the hidden layers and hidden neurons on our own

# # define hidden layers and neuron in each layer
# number_of_hidden_layers = 2
# neuron_hidden_layer_1 = 20
# neuron_hidden_layer_2 = 10

#https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/
# define base model
def baseline_model():
	# create model
	model = Sequential()
	model.add(Dense(input_neurons, input_dim=input_neurons, kernel_initializer='normal', activation='relu'))
	model.add(Dense(1, kernel_initializer='normal'))
	# Compile model
	model.compile(loss='mean_squared_error', optimizer='adam')
	return model

from keras.wrappers.scikit_learn import KerasRegressor
# importing the sequential model
from keras.models import Sequential
# importing different layers from keras
from keras.layers import InputLayer, Dense 

estimator = KerasRegressor(build_fn=baseline_model, epochs=30, batch_size=5, verbose=0)

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
kfold = KFold(n_splits=5)
results = cross_val_score(estimator, X_train, y_train, cv=kfold)
print("Results: %.2f (%.2f) MSE" % (results.mean(), results.std()))

# Results: -1192392.75 (80768.37) MSE

# model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)

"""## 6. Evaluating model performance on validation set"""

def mean_absolute_percentage_error(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

estimator.fit(X_train, y_train)
prediction = estimator.predict(X_test)
mean_absolute_percentage_error(y_test, prediction) #54.58289499388174

# Save the entire model as a SavedModel.
!mkdir -p saved_model
estimator.model.save('saved_model/my_model')

from keras.models import load_model

# Instantiate the model as you please (we are not going to use this)
model2 = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=10, verbose=1)

# This is where you load the actual saved model into new variable.
model2.model = load_model('saved_model/my_model')

# Now you can use this to predict on new data (without fitting model2, because it uses the older saved model)
model2.model.summary()

"""# Predicting on Test"""

test = pd.read_csv('test_FewQE9B.csv')
print(test.shape)

print(test.isnull().sum())
test['Outlet_Size'].fillna(test['Outlet_Size'].mode()[0], inplace=True)
# filling missing values of continuous variables with mean
test['Item_Weight'].fillna(test['Item_Weight'].mean(), inplace=True)
cat_cols = test.select_dtypes(include=['object']).columns.tolist()
# print(cat_cols)
cat_cols.remove('Item_Identifier')
cat_cols.remove('Outlet_Identifier')
cat_cols.remove('Item_Fat_Content')
print(cat_cols)
# converting the categories into numbers using map function
dict1 = {'Low Fat': 0, 'low fat': 0, 'LF':0, 'Regular':1, 'reg': 1}
test['Item_Fat_Content'] = test['Item_Fat_Content'].map(dict1)
test_num_cols = test.select_dtypes(include=['float','int']).columns.tolist()
print(test_num_cols)
test_item_id = test["Item_Identifier"].copy()
test_outlet_id = test["Outlet_Identifier"].copy()
model_df = test.drop(["Item_Identifier","Outlet_Identifier"], axis = 1)

print(model_df.shape)
model_df.head(2)

cat_cols

# Convert categorical variables to numeric - one hot encoder

from sklearn.preprocessing import OneHotEncoder


test_cat_encoder = OneHotEncoder(sparse=False)
model_cat = model_df[cat_cols]

model_cat_1hot = test_cat_encoder.fit_transform(model_cat)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
    ('std_scaler', StandardScaler())
])

model_num = model_df.drop(cat_cols, axis = 1)
model_num_tr = num_pipeline.fit_transform(model_num)

model_num_tr.shape

from sklearn.compose import ColumnTransformer

full_pipeline = ColumnTransformer([
    ('num', num_pipeline, test_num_cols),
    ('cat', OneHotEncoder(sparse=False), cat_cols)
])

test_model_prepared = full_pipeline.fit_transform(model_df)

len(test_num_cols) + len(test_cat_encoder.get_feature_names().tolist())

test_trans_df = pd.DataFrame(
    test_model_prepared,
    columns=test_num_cols + test_cat_encoder.get_feature_names().tolist())

test_trans_df.head()

# test_trans_df["Item_Identifier"] = test_item_id
# test_trans_df["Outlet_Identifier"] = test_outlet_id
# cols = list(test_trans_df)
# cols.insert(0, cols.pop(cols.index('Outlet_Identifier')))
# cols.insert(0, cols.pop(cols.index('Item_Identifier')))
# test_trans_df = test_trans_df.loc[:, cols]
# test_trans_df.head()

"""# Predicting on test"""

estimator.fit(X, y)
prediction = estimator.predict(test_trans_df)
prediction

sample_sub.head()

sub1 = pd.DataFrame(prediction, columns= ['Item_Outlet_Sales'])
sub1["Item_Identifier"] = test_item_id
sub1["Outlet_Identifier"] = test_outlet_id
sub1.head()

sub1 = sub1[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']]
# saving the predicted data
sub1.to_csv('bigmart_sub1.csv', index=False)

sub1.head()

sub = pd.read_csv('bigmart_sub1.csv')

sub1.describe()

impute_mean = sub1["Item_Outlet_Sales"].mean()

sub1["Item_Outlet_Sales"] = np.where(sub1["Item_Outlet_Sales"] <0, impute_mean, sub1["Item_Outlet_Sales"])

sub1.to_csv('bigmart_sub1.csv', index=False)

sub = pd.read_csv('bigmart_sub1.csv')

