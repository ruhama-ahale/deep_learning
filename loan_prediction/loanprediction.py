# -*- coding: utf-8 -*-
"""loanprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xm5igO90Up_q8TtGaIzr-Ghd_0VyQ_zL
"""

import numpy as np
print(np.__version__)

"""# Pre-processing Steps

<ol>1. Filling the missing values</ol>
<ol>2. Converting categories to numbers</ol>
<ol>3. Bring all the variables in range 0 to 1</ol>
"""

# importing required libraries
import pandas as pd
# check version on pandas
print('Version of pandas:', pd.__version__)

# reading the loan prediction data
data = pd.read_csv('loan_data.csv')

data.head()

data.shape

# checking missing values in the data
data.isnull().sum()

# data types of the variables
data.dtypes

#Multiple records of loanids?
data.groupby('Loan_ID').size().sort_values(ascending=False).head() #No

## 1. Filling the missing values



"""## 1. Filling the missing values

### Categorical Data: Mode
"""

# filling missing values of categorical variables with mode

data['Gender'].fillna(data['Gender'].mode()[0], inplace=True)

data['Married'].fillna(data['Married'].mode()[0], inplace=True)

data['Dependents'].fillna(data['Dependents'].mode()[0], inplace=True)

data['Self_Employed'].fillna(data['Self_Employed'].mode()[0], inplace=True)

data['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].mode()[0], inplace=True)

data['Credit_History'].fillna(data['Credit_History'].mode()[0], inplace=True)

"""### Continuous Data: Mean"""

# filling missing values of continuous variables with mean
data['LoanAmount'].fillna(data['LoanAmount'].mean(), inplace=True)

# checking missing values after imputation
data.isnull().sum()



"""## 2. Converting categories to numbers"""

# converting the categories into numbers using map function
data['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})
data['Married'] = data['Married'].map({'No': 0, 'Yes': 1})
data['Dependents'] = data['Dependents'].map({'0': 0, '1': 1, '2': 2, '3+': 3})
data['Education'] = data['Education'].map({'Graduate': 1, 'Not Graduate': 0})
data['Self_Employed'] = data['Self_Employed'].map({'No': 0, 'Yes': 1})
data['Property_Area'] = data['Property_Area'].map({'Rural': 0, 'Semiurban': 1, 'Urban': 2})
data['Loan_Status'] = data['Loan_Status'].map({'N': 0, 'Y': 1})

"""## 3. Bringing all the variables in range 0 to 1

<img src='https://drive.google.com/uc?id=1Z7wN2BE8CDSYIJly5l0K9uqYdj9WUo3f'>
"""

# bringing variables in the range 0 to 1
data['Dependents']=(data['Dependents']-data['Dependents'].min())/(data['Dependents'].max()-data['Dependents'].min())

# applying for loop to bring all the variables in range 0 to 1

for i in data.columns[1:]:
    data[i] = (data[i] - data[i].min()) / (data[i].max() - data[i].min())

# again looking at first five rows of pre-processed data
data.head()

# saving the pre-processed data
data.to_csv('loan_prediction_data.csv', index=False)



"""#Solving_Loan_Prediction_problem_using_Neural_Network"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

"""# Steps to build a Neural Network using Keras

<ol>1. Loading the dataset</ol>
<ol>2. Creating training and validation set</ol>
<ol>3. Defining the architecture of the model</ol>
<ol>4. Compiling the model (defining loss function, optimizer)</ol>
<ol>5. Training the model</ol>
<ol>6. Evaluating model performance on training and validation set</ol>

## 1. Loading the dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# importing the required libraries
import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
# %matplotlib inline

# check version on sklearn
print('Version of sklearn:', sklearn.__version__)

# loading the pre-processed dataset
data = pd.read_csv('loan_prediction_data.csv')

# looking at the first five rows of the dataset
data.head()

# checking missing values
data.isnull().sum()

# checking the data type
data.dtypes

# removing the loan_ID since these are just the unique values
data = data.drop('Loan_ID', axis=1)

# looking at the shape of the data
data.shape

# separating the independent and dependent variables

# storing all the independent variables as X
X = data.drop('Loan_Status', axis=1)

# storing the dependent variable as y
y = data['Loan_Status']

# shape of independent and dependent variables
X.shape, y.shape

"""## 2. Creating training and validation set"""

# Creating training and validation set

# stratify will make sure that the distribution of classes in train and validation set it similar
# random state to regenerate the same train and validation set
# test size 0.2 will keep 20% data in validation and remaining 80% in train set

X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=data['Loan_Status'],random_state=10,test_size=0.2)

# shape of training and validation set
(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)

"""## 3. Defining the architecture of the model"""

# checking the version of keras
import keras
print(keras.__version__)

# checking the version of tensorflow
import tensorflow as tf
print(tf.__version__)

"""### a. Create a model

<img src='https://drive.google.com/uc?id=1iZNZ3kwSHRNf-Irn3DZmMuBb6K-Lro7w'>
"""

# importing the sequential model
from keras.models import Sequential

"""### b. Defining different layers

<img src='https://drive.google.com/uc?id=16X6De2hua1XJBe3dfmUUeGTgP6PbXEpc'>
"""

# importing different layers from keras
from keras.layers import InputLayer, Dense

"""<img src='https://drive.google.com/uc?id=1tsy4B6G0UN4-J4L4roOdoWQiZMUdgw2a'>"""

# number of input neurons
X_train.shape

# number of features in the data
X_train.shape[1]

# defining input neurons
input_neurons = X_train.shape[1]

"""<img src='https://drive.google.com/uc?id=1xL_hM9rGItZjsZ8Lofwzw_9fZUi4bgJo'>"""

# number of output neurons

# since loan prediction is a binary classification problem, we will have single neuron in the output layer

# define number of output neurons
output_neurons = 1

# number of hidden layers and hidden neurons

# It is a hyperparameter and we can pick the hidden layers and hidden neurons on our own

# define hidden layers and neuron in each layer
number_of_hidden_layers = 2
neuron_hidden_layer_1 = 10
neuron_hidden_layer_2 = 5

# activation function of different layers

# for now I have picked relu as an activation function for hidden layers, you can change it as well
# since it is a binary classification problem, I have used sigmoid activation function in the final layer

# defining the architecture of the model
model = Sequential()
model.add(InputLayer(input_shape=(input_neurons,)))
model.add(Dense(units=neuron_hidden_layer_1, activation='relu'))
model.add(Dense(units=neuron_hidden_layer_2, activation='relu'))
model.add(Dense(units=output_neurons, activation='sigmoid'))

# summary of the model
model.summary()

# number of parameters between input and first hidden layer

input_neurons*neuron_hidden_layer_1

# number of parameters between input and first hidden layer

# adding the bias for each neuron of first hidden layer

input_neurons*neuron_hidden_layer_1 + 10

# number of parameters between first and second hidden layer

neuron_hidden_layer_1*neuron_hidden_layer_2 + 5

# number of parameters between second hidden and output layer

neuron_hidden_layer_2*output_neurons + 1

"""## 4. Compiling the model (defining loss function, optimizer)"""

# compiling the model

# loss as binary_crossentropy, since we have binary classification problem
# defining the optimizer as adam
# Evaluation metric as accuracy

model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])

"""## 5. Training the model"""

# training the model

# passing the independent and dependent features for training set for training the model

# validation data will be evaluated at the end of each epoch

# setting the epochs as 50

# storing the trained model in model_history variable which will be used to visualize the training process

model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)

"""## 6. Evaluating model performance on validation set"""

# getting predictions for the validation set
prediction = model.predict_classes(X_test)

# calculating the accuracy on validation set
accuracy_score(y_test, prediction)

"""### Visualizing the model performance"""

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# summarize history for accuracy
plt.plot(model_history.history['acc'])
plt.plot(model_history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()